{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjktIs4BnY1sxR43ANqMa7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SPARSHTHALYARI/WebSrcapper/blob/new%2Cupdated%2Cwith-Sentiment-Analyze/webScrapper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download the \"punkt\" tokenizer model\n",
        "nltk.download('punkt')\n",
        "\n",
        "import nltk\n",
        "\n",
        "# Download the \"stopwords\" resource\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3dYU91yYNie",
        "outputId": "502581f8-242c-4506-d36b-bb0d8a440343"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "\n",
        "# Define the URL of the webpage you want to scrape\n",
        "url = 'https://insights.blackcoffer.com/rise-of-telemedicine-and-its-impact-on-livelihood-by-2040-3-2/'\n",
        "\n",
        "# Send an HTTP GET request to the URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful (status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Parse the HTML content of the webpage\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find the main content of the article\n",
        "    article_content = soup.find('div', class_='td-post-content')\n",
        "\n",
        "    if article_content:\n",
        "        # Extract the article title (usually within an <h1> or <h2> tag)\n",
        "        try:\n",
        "            article_title = article_content.find('h1').get_text()\n",
        "        except AttributeError:\n",
        "            article_title = \"Untitled\"  # Use a default title if <h1> is not found\n",
        "\n",
        "        # Extract the article text from the main content\n",
        "        article_text = \"\"\n",
        "        for paragraph in article_content.find_all('p'):\n",
        "            article_text += paragraph.get_text() + \"\\n\"\n",
        "\n",
        "        # Create a DataFrame for analysis results\n",
        "        analysis_results = {\n",
        "            \"Article Title\": [article_title],\n",
        "            \"Article Text\": [article_text]\n",
        "        }\n",
        "        df = pd.DataFrame(analysis_results)\n",
        "\n",
        "        # Tokenize the text into words\n",
        "        words = nltk.word_tokenize(article_text)\n",
        "\n",
        "        # Remove stop words (common words like \"the,\" \"and,\" \"is,\" etc.)\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "        # Calculate word frequency\n",
        "        word_freq = nltk.FreqDist(filtered_words)\n",
        "\n",
        "        # Analyze sentiment using TextBlob\n",
        "        text_blob = TextBlob(article_text)\n",
        "        sentiment_score = text_blob.sentiment.polarity\n",
        "\n",
        "        # Extract keywords (most common words)\n",
        "        num_keywords = 10  # Specify the number of top keywords to extract\n",
        "        keywords = [word for word, freq in word_freq.most_common(num_keywords)]\n",
        "\n",
        "        # Add analysis results to the DataFrame\n",
        "        df[\"Most Common Words\"] = [keywords]\n",
        "        df[\"Sentiment Score\"] = [sentiment_score]\n",
        "\n",
        "        # Save the DataFrame to an XLSX file\n",
        "        df.to_excel('Output Data Structure.xlsx', index=False)\n",
        "\n",
        "        print(\"Analysis results have been saved to 'analysis_results.xlsx'.\")\n",
        "\n",
        "    else:\n",
        "        print(\"Failed to find the article content on the webpage.\")\n",
        "\n",
        "else:\n",
        "    print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHBT2Me-pdS3",
        "outputId": "be101e2c-8963-4585-e120-58e03fa3a20a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analysis results have been saved to 'analysis_results.xlsx'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openpyxl\n",
        "import nltk\n",
        "from textblob import TextBlob\n",
        "import re\n",
        "\n",
        "# Define a function to count syllables in a word\n",
        "def count_syllables(word):\n",
        "    vowels = \"aeiouyAEIOUY\"\n",
        "    count = 0\n",
        "    prev_char = None\n",
        "    for char in word:\n",
        "        if char in vowels and (prev_char is None or prev_char not in vowels):\n",
        "            count += 1\n",
        "        prev_char = char\n",
        "    if word.endswith(\"e\"):\n",
        "        count -= 1\n",
        "    if count == 0:\n",
        "        count = 1  # At least one syllable for a word\n",
        "    return count\n",
        "\n",
        "# Load the Excel file\n",
        "xlsx_file = 'Output Data Structure.xlsx'\n",
        "workbook = openpyxl.load_workbook(xlsx_file)\n",
        "sheet = workbook.active\n",
        "\n",
        "# Initialize variables\n",
        "positive_score = 0\n",
        "negative_score = 0\n",
        "total_polarity_score = 0\n",
        "total_subjectivity_score = 0\n",
        "total_sentence_length = 0\n",
        "total_complex_words = 0\n",
        "total_word_count = 0\n",
        "total_syllables = 0\n",
        "total_personal_pronouns = 0\n",
        "total_word_length = 0\n",
        "\n",
        "# Iterate through rows in the Excel sheet to calculate variables\n",
        "for row in sheet.iter_rows(min_row=2, values_only=True):\n",
        "    # Extract the text from the Excel sheet (assuming it's in the second column)\n",
        "    text = row[1]\n",
        "\n",
        "    # Check if the cell contains a non-empty string\n",
        "    if isinstance(text, str) and text.strip():\n",
        "        # Tokenize the text into sentences\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "        for sentence in sentences:\n",
        "            # Calculate polarity and subjectivity scores for the sentence\n",
        "            blob = TextBlob(sentence)\n",
        "            polarity_score = blob.sentiment.polarity\n",
        "            subjectivity_score = blob.sentiment.subjectivity\n",
        "\n",
        "            # Calculate sentence length (number of words)\n",
        "            words = nltk.word_tokenize(sentence)\n",
        "            sentence_length = len(words)\n",
        "\n",
        "            # Count complex words (words with more than 2 syllables)\n",
        "            complex_words = [word for word in words if count_syllables(word) > 2]\n",
        "\n",
        "            # Calculate syllables and word count for the sentence\n",
        "            syllables = sum([count_syllables(word) for word in words])\n",
        "            word_count = len(words)\n",
        "\n",
        "            # Count personal pronouns (you can extend the list of pronouns as needed)\n",
        "            personal_pronouns = re.findall(r'\\b(I|me|my|mine|we|us|our|ours|you|your|yours|he|him|his|she|her|hers|it|its|they|them|their|theirs)\\b', sentence, flags=re.IGNORECASE)\n",
        "\n",
        "            # Calculate average word length for the sentence\n",
        "            word_length = sum([len(word) for word in words]) / word_count if word_count > 0 else 0\n",
        "\n",
        "            # Update total scores and counts\n",
        "            positive_score += max(0, polarity_score)\n",
        "            negative_score += max(0, -polarity_score)\n",
        "            total_polarity_score += polarity_score\n",
        "            total_subjectivity_score += subjectivity_score\n",
        "            total_sentence_length += sentence_length\n",
        "            total_complex_words += len(complex_words)\n",
        "            total_word_count += word_count\n",
        "            total_syllables += syllables\n",
        "            total_personal_pronouns += len(personal_pronouns)\n",
        "            total_word_length += word_length\n",
        "\n",
        "# Calculate average values\n",
        "num_sentences = sheet.max_row - 1  # Subtract header row\n",
        "avg_sentence_length = total_sentence_length / num_sentences if num_sentences > 0 else 0\n",
        "percentage_of_complex_words = (total_complex_words / total_word_count) * 100 if total_word_count > 0 else 0\n",
        "fog_index = 0.4 * (avg_sentence_length + percentage_of_complex_words)\n",
        "avg_number_of_words_per_sentence = total_word_count / num_sentences if num_sentences > 0 else 0\n",
        "syllable_per_word = total_syllables / total_word_count if total_word_count > 0 else 0\n",
        "personal_pronouns = total_personal_pronouns\n",
        "\n",
        "# Print the calculated variables\n",
        "print(\"POSITIVE SCORE:\", positive_score)\n",
        "print(\"NEGATIVE SCORE:\", negative_score)\n",
        "print(\"POLARITY SCORE:\", total_polarity_score)\n",
        "print(\"SUBJECTIVITY SCORE:\", total_subjectivity_score)\n",
        "print(\"AVG SENTENCE LENGTH:\", avg_sentence_length)\n",
        "print(\"PERCENTAGE OF COMPLEX WORDS:\", percentage_of_complex_words)\n",
        "print(\"FOG INDEX:\", fog_index)\n",
        "print(\"AVG NUMBER OF WORDS PER SENTENCE:\", avg_number_of_words_per_sentence)\n",
        "print(\"COMPLEX WORD COUNT:\", total_complex_words)\n",
        "print(\"WORD COUNT:\", total_word_count)\n",
        "print(\"SYLLABLE PER WORD:\", syllable_per_word)\n",
        "print(\"PERSONAL PRONOUNS:\", personal_pronouns)\n",
        "print(\"AVG WORD LENGTH:\", total_word_length / total_word_count if total_word_count > 0 else 0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pz1ac6KrfHu",
        "outputId": "ac053242-123b-44d3-d4e9-5bdd0329f338"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POSITIVE SCORE: 10.167209595959596\n",
            "NEGATIVE SCORE: 1.9416666666666667\n",
            "POLARITY SCORE: 8.225542929292928\n",
            "SUBJECTIVITY SCORE: 31.270112179487175\n",
            "AVG SENTENCE LENGTH: 1837.0\n",
            "PERCENTAGE OF COMPLEX WORDS: 20.30484485574306\n",
            "FOG INDEX: 742.9219379422973\n",
            "AVG NUMBER OF WORDS PER SENTENCE: 1837.0\n",
            "COMPLEX WORD COUNT: 373\n",
            "WORD COUNT: 1837\n",
            "SYLLABLE PER WORD: 1.7729994556341862\n",
            "PERSONAL PRONOUNS: 46\n",
            "AVG WORD LENGTH: 0.22102858574349651\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openpyxl\n",
        "import csv\n",
        "\n",
        "# Function to remove problematic characters from a string\n",
        "def remove_problematic_chars(text):\n",
        "    # Define a set of characters that should be removed\n",
        "    problematic_chars = set(\"\u0006+lìflµ5cck$}û9^2¶\u0007ØÑÒïO@­òB¹/Áy\fd0ÞM¶\u001f¢P~ÍD^DuD+c\u0012Cjî]°Ò3\u001cÀKu\u0007ó\u0007°HRK0\u0003\u000b¿\u0010Y×j%T@I.\\ðZ-xÿ\u0019ú\fÓ\")\n",
        "    return ''.join(char for char in text if char not in problematic_chars)\n",
        "\n",
        "# Create a new Excel workbook\n",
        "workbook = openpyxl.Workbook()\n",
        "sheet = workbook.active\n",
        "\n",
        "# Define column headers\n",
        "headers = [\n",
        "    \"POSITIVE SCORE\",\n",
        "    \"NEGATIVE SCORE\",\n",
        "    \"POLARITY SCORE\",\n",
        "    \"SUBJECTIVITY SCORE\",\n",
        "    \"AVG SENTENCE LENGTH\",\n",
        "    \"PERCENTAGE OF COMPLEX WORDS\",\n",
        "    \"FOG INDEX\",\n",
        "    \"AVG NUMBER OF WORDS PER SENTENCE\",\n",
        "    \"COMPLEX WORD COUNT\",\n",
        "    \"WORD COUNT\",\n",
        "    \"SYLLABLE PER WORD\",\n",
        "    \"PERSONAL PRONOUNS\",\n",
        "    \"AVG WORD LENGTH\",\n",
        "]\n",
        "\n",
        "# Write headers to the first row of the spreadsheet\n",
        "for col_num, header in enumerate(headers, start=1):\n",
        "    sheet.cell(row=1, column=col_num, value=header)\n",
        "\n",
        "# Specify the full file path to your CSV data file\n",
        "csv_file_path = '/content/Output Data Structure.xlsx'\n",
        "\n",
        "# Read data from the CSV file with 'latin-1' encoding and 'ignore' error handling\n",
        "with open(csv_file_path, 'r', encoding='latin-1', errors='ignore') as csv_file:\n",
        "    # Preprocess each line to remove problematic characters\n",
        "    cleaned_lines = [remove_problematic_chars(line) for line in csv_file]\n",
        "\n",
        "    # Use the cleaned lines to create a CSV reader\n",
        "    csv_reader = csv.reader(cleaned_lines)\n",
        "    next(csv_reader)  # Skip the header row if it exists\n",
        "\n",
        "    # Populate the spreadsheet with data from the CSV file\n",
        "    for row_num, row_data in enumerate(csv_reader, start=2):\n",
        "        for col_num, cell_value in enumerate(row_data, start=1):\n",
        "            sheet.cell(row=row_num, column=col_num, value=cell_value)\n",
        "\n",
        "# Specify the full file path for saving the Excel spreadsheet\n",
        "xlsx_file_path = '/content/path'\n",
        "\n",
        "# Save the spreadsheet to the specified file path\n",
        "workbook.save(xlsx_file_path)\n",
        "\n",
        "print(f\"Excel spreadsheet '{xlsx_file_path}' has been created and populated with data from '{csv_file_path}'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "eQgLeBtqYF-F",
        "outputId": "9eda4697-9ebd-4898-f321-70d5b492bb2b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-e614d9e20147>\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin-1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# Preprocess each line to remove problematic characters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mcleaned_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mremove_problematic_chars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# Use the cleaned lines to create a CSV reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mError\u001b[0m: line contains NUL"
          ]
        }
      ]
    }
  ]
}